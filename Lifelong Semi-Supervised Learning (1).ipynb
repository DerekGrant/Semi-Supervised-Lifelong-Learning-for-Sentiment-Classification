{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from string import digits\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "import string\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_name(file_dir): \n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        print(root) #当前目录路径\n",
    "        print(dirs) #当前路径下所有子目录\n",
    "        print(files) #当前路径下所有非目录子文件\n",
    "        return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\data\n",
      "[]\n",
      "['AlarmClock.txt', 'Baby.txt', 'Bag.txt', 'CableModem.txt', 'Dumbbell.txt', 'Flashlight.txt', 'Gloves.txt', 'GPS.txt', 'GraphicsCard.txt', 'Headphone.txt', 'HomeTheaterSystem.txt', 'Jewelry.txt', 'Keyboard.txt', 'Magazine_Subscriptions.txt', 'Movies_TV.txt', 'Projector.txt', 'RiceCooker.txt', 'Sandal.txt', 'Vacuum.txt', 'Video_Games.txt']\n"
     ]
    }
   ],
   "source": [
    "files = file_name('..\\data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanLines(line):\n",
    "    return line.lower() #换小写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countFrequence(raw):\n",
    "    review = raw['Review']\n",
    "    label = raw['Label']\n",
    "    \n",
    "    words = CleanLines(review).split()#换小写，按空格切分\n",
    "                \n",
    "    for word in words:\n",
    "        #计算词频\n",
    "        try:\n",
    "            wordFreq.update({word:wordFreq.get(word)+1})\n",
    "        except:\n",
    "            wordFreq.update({word:1})\n",
    "        #计算在正负类中的词频\n",
    "        if label == 'POS':\n",
    "            try:\n",
    "                posFreq.update({word:posFreq.get(word)+1})\n",
    "            except:\n",
    "                posFreq.update({word:1})\n",
    "        elif label == 'NEG': \n",
    "            try:\n",
    "                negFreq.update({word:negFreq.get(word)+1})\n",
    "            except:\n",
    "                negFreq.update({word:1})                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用贝叶斯概率计算一个评论是正类或负类的概率\n",
    "def calculateProba(review):\n",
    "    words = CleanLines(review).split()\n",
    "    \n",
    "    posProba = math.log10(posPriorProb)\n",
    "    negProba = math.log10(negPriorProb)\n",
    "    for word in words:\n",
    "        #如果当前词未经过训练，赋默认值\n",
    "        wordPosProba = posProb.get(word)\n",
    "        if wordPosProba is None:\n",
    "            posProba += math.log10((lambdaSmoothing + 0)/(lambdaSmoothing*len(vocabulary) + posTotalFreq)) ##\n",
    "        else:\n",
    "            posProba += math.log10(wordPosProba)\n",
    "         \n",
    "        wordNegProba = negProb.get(word)\n",
    "        if wordNegProba is None:\n",
    "            negProba += math.log10((lambdaSmoothing + 0)/(lambdaSmoothing*len(vocabulary) + negTotalFreq))\n",
    "        else:\n",
    "            negProba += math.log10(wordNegProba)\n",
    "    try:\n",
    "        return (1-10**(negProba-posProba))/(1+10**(negProba-posProba)) ##normalize to -1,1\n",
    "    except:\n",
    "        if(negProba-posProba>20):\n",
    "            return (1-10**int(negProba-posProba))/(1+10**int(negProba-posProba)) ##normalize to -1,1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Bag.txt with  3000  rows.\n"
     ]
    }
   ],
   "source": [
    "posCommon = {}\n",
    "negCommon = {}\n",
    "startID = 3\n",
    "file = files[0]\n",
    "trainData = pd.read_csv(file,sep='\\t')\n",
    "\n",
    "#initial learning\n",
    "for file in files[1:startID]:\n",
    "    trainData = pd.concat([trainData,pd.read_csv(file,sep='\\t')])\n",
    "\n",
    "    \n",
    "ratio = 0.2#只选对分类影响系数前两成的词汇\n",
    "\n",
    "dataRow = trainData.shape[0]\n",
    "print('******************************************************************')\n",
    "print('ratio is: ',ratio)\n",
    "print('Processing ',file,'with ',dataRow,' rows.')\n",
    "X_train = trainData['Review']\n",
    "y_train = trainData['Label']\n",
    "\n",
    "\n",
    "#print('Shape of trainData: ',trainData.shape)\n",
    "#print('Shape of testData: ',testData.shape)\n",
    "\n",
    "wordFreq = {}\n",
    "posFreq = {}\n",
    "negFreq = {}\n",
    "buffer =  trainData.apply(lambda X: countFrequence(X),axis =1)\n",
    "#print('TrainData counting completed')\n",
    "\n",
    "posVoca = list(posFreq.keys())\n",
    "negVoca = list(negFreq.keys())\n",
    "vocabulary = set(posVoca + negVoca)\n",
    "\n",
    "posTotalFreq = 0\n",
    "for word in posVoca:\n",
    "    posTotalFreq += posFreq.get(word)\n",
    "\n",
    "negTotalFreq = 0\n",
    "for word in negVoca:\n",
    "    negTotalFreq += negFreq.get(word)\n",
    "\n",
    "lambdaSmoothing = 1\n",
    "\n",
    "posProb = {}\n",
    "\n",
    "for word in vocabulary:\n",
    "    frequence = posFreq.get(word)\n",
    "    if frequence is None:\n",
    "        frequence = 0\n",
    "\n",
    "    posProb.update({word: (lambdaSmoothing + frequence)/(lambdaSmoothing*len(vocabulary) + posTotalFreq)})\n",
    "\n",
    "negProb = {}\n",
    "wordProb = {}\n",
    "\n",
    "for word in vocabulary:\n",
    "    frequence = negFreq.get(word)\n",
    "    if frequence is None:\n",
    "        frequence = 0\n",
    "\n",
    "    negProb.update({word: (lambdaSmoothing + frequence)/(lambdaSmoothing*len(vocabulary) + negTotalFreq)})\n",
    "\n",
    "totalWords = sum(posFreq.values()) + sum(negFreq.values())\n",
    "\n",
    "for word in vocabulary:\n",
    "    try: \n",
    "        frequence = posFreq.get(word) + 0\n",
    "        try:\n",
    "            frequence += negFreq.get(word)\n",
    "        except:\n",
    "            pass\n",
    "    except:\n",
    "        frequence = negFreq.get(word)\n",
    "    wordProb.update({word:frequence/totalWords})\n",
    "\n",
    "\n",
    "## To calculate the orientation of words 计算词汇对分类的影响程度\n",
    "orientation = {}\n",
    "for word in vocabulary:\n",
    "    if wordFreq.get(word) > 5*startID: #只出现一次的词偶然性太大，只选择在每个数据集中至少出现过5次及以上的词\n",
    "        orientation.update({word:posProb.get(word)/wordProb.get(word)})\n",
    "    else:\n",
    "        orientation.update({word:1})\n",
    "\n",
    "\n",
    "#sorted() small to large, smaller means more negative \n",
    "threshold1 = (sorted(orientation.values()))[math.ceil(len(orientation)*ratio/2)] \n",
    "threshold2 = (sorted(orientation.values()))[-math.ceil(len(orientation)*ratio/2)]\n",
    "#print('threshold is: ', threshold)\n",
    "#totalThreshold += threshold\n",
    "\n",
    "#Only use the words whose orientation larger than threshold\n",
    "for word in posProb:\n",
    "    if (orientation.get(word) > threshold1) and  (orientation.get(word) < threshold2):\n",
    "        posProb.update({word: (lambdaSmoothing + 0)/(lambdaSmoothing*len(vocabulary) + posTotalFreq)})\n",
    "\n",
    "for word in negFreq:\n",
    "    if (orientation.get(word) > threshold1) and  (orientation.get(word) < threshold2):\n",
    "        negProb.update({word: (lambdaSmoothing + 0)/(lambdaSmoothing*len(vocabulary) + negTotalFreq)})\n",
    "\n",
    "negValues = np.array((sorted(orientation.values())))[:math.ceil(len(orientation)*ratio/2)]\n",
    "posValues  = np.array((sorted(orientation.values())))[-math.ceil(len(orientation)*ratio/2):]\n",
    "\n",
    "for word in vocabulary:\n",
    "    if (orientation.get(word) > threshold2):\n",
    "        try:\n",
    "            weight = 1 + np.where(posValues==orientation.get(word))[0][0]/len(posValues)\n",
    "            posCommon.update({word:posCommon.get(word)+weight})\n",
    "        except:\n",
    "            posCommon.update({word:1})\n",
    "    elif (orientation.get(word) < threshold1):\n",
    "        try:\n",
    "            weight = 1 + -(np.where(negValues==orientation.get(word))[0][0]-len(posValues))/len(posValues)\n",
    "            negCommon.update({word:negCommon.get(word)+weight})\n",
    "        except:\n",
    "            negCommon.update({word:1})\n",
    "\n",
    "posNum = sum(trainData['Label'] == 'POS')\n",
    "negNum = sum(trainData['Label'] == 'NEG')\n",
    "posPriorProb = posNum/(posNum+negNum)\n",
    "negPriorProb = negNum/(posNum+negNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  CableModem.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  CableModem.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.94      0.80      0.87       121\n",
      "     class 1       0.97      0.99      0.98       845\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       966\n",
      "   macro avg       0.96      0.90      0.92       966\n",
      "weighted avg       0.97      0.97      0.97       966\n",
      "\n",
      "0.924253512880562\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Dumbbell.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Dumbbell.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.93      0.73      0.82       146\n",
      "     class 1       0.95      0.99      0.97       764\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       910\n",
      "   macro avg       0.94      0.86      0.89       910\n",
      "weighted avg       0.95      0.95      0.94       910\n",
      "\n",
      "0.8923076923076922\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Flashlight.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Flashlight.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.90      0.66      0.76       108\n",
      "     class 1       0.96      0.99      0.97       816\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       924\n",
      "   macro avg       0.93      0.82      0.87       924\n",
      "weighted avg       0.95      0.95      0.95       924\n",
      "\n",
      "0.8661330877926126\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Gloves.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Gloves.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.92      0.57      0.70       127\n",
      "     class 1       0.93      0.99      0.96       796\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       923\n",
      "   macro avg       0.93      0.78      0.83       923\n",
      "weighted avg       0.93      0.93      0.93       923\n",
      "\n",
      "0.8326332842853108\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  GPS.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  GPS.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.85      0.91      0.88       179\n",
      "     class 1       0.98      0.96      0.97       739\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       918\n",
      "   macro avg       0.92      0.94      0.93       918\n",
      "weighted avg       0.95      0.95      0.95       918\n",
      "\n",
      "0.9255337192581394\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  GraphicsCard.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  GraphicsCard.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.90      0.75      0.82       136\n",
      "     class 1       0.96      0.99      0.97       797\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       933\n",
      "   macro avg       0.93      0.87      0.90       933\n",
      "weighted avg       0.95      0.95      0.95       933\n",
      "\n",
      "0.8957238974450678\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Headphone.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Headphone.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.86      0.79      0.82       187\n",
      "     class 1       0.95      0.97      0.96       704\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       891\n",
      "   macro avg       0.90      0.88      0.89       891\n",
      "weighted avg       0.93      0.93      0.93       891\n",
      "\n",
      "0.8901199357158658\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  HomeTheaterSystem.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  HomeTheaterSystem.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.92      0.93      0.92       261\n",
      "     class 1       0.97      0.97      0.97       644\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       905\n",
      "   macro avg       0.95      0.95      0.95       905\n",
      "weighted avg       0.96      0.96      0.96       905\n",
      "\n",
      "0.9462799615352653\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Jewelry.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Jewelry.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.90      0.57      0.70       110\n",
      "     class 1       0.94      0.99      0.97       791\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       901\n",
      "   macro avg       0.92      0.78      0.83       901\n",
      "weighted avg       0.94      0.94      0.93       901\n",
      "\n",
      "0.8333538840937115\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Keyboard.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Keyboard.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.91      0.80      0.85       203\n",
      "     class 1       0.94      0.98      0.96       693\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       896\n",
      "   macro avg       0.93      0.89      0.90       896\n",
      "weighted avg       0.94      0.94      0.93       896\n",
      "\n",
      "0.9049984095715888\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Magazine_Subscriptions.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Magazine_Subscriptions.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.78      0.96      0.86       247\n",
      "     class 1       0.99      0.90      0.94       672\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       919\n",
      "   macro avg       0.88      0.93      0.90       919\n",
      "weighted avg       0.93      0.92      0.92       919\n",
      "\n",
      "0.9016104311763247\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Movies_TV.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Movies_TV.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.82      0.72      0.77       101\n",
      "     class 1       0.97      0.98      0.97       829\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       930\n",
      "   macro avg       0.89      0.85      0.87       930\n",
      "weighted avg       0.95      0.95      0.95       930\n",
      "\n",
      "0.8710368736211787\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Projector.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Projector.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.87      0.77      0.82       186\n",
      "     class 1       0.94      0.97      0.96       733\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       919\n",
      "   macro avg       0.91      0.87      0.89       919\n",
      "weighted avg       0.93      0.93      0.93       919\n",
      "\n",
      "0.8870660522273426\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  RiceCooker.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  RiceCooker.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.91      0.73      0.81       175\n",
      "     class 1       0.94      0.98      0.96       764\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       939\n",
      "   macro avg       0.93      0.86      0.89       939\n",
      "weighted avg       0.94      0.94      0.93       939\n",
      "\n",
      "0.8874752460165127\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Sandal.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Sandal.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      0.65      0.77       115\n",
      "     class 1       0.95      1.00      0.97       835\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       950\n",
      "   macro avg       0.95      0.82      0.87       950\n",
      "weighted avg       0.95      0.95      0.95       950\n",
      "\n",
      "0.8737022757762174\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Vacuum.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Vacuum.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.88      0.86      0.87       203\n",
      "     class 1       0.96      0.97      0.96       717\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       920\n",
      "   macro avg       0.92      0.91      0.92       920\n",
      "weighted avg       0.94      0.94      0.94       920\n",
      "\n",
      "0.9154996456045432\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Video_Games.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Video_Games.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.86      0.91      0.88       190\n",
      "     class 1       0.98      0.96      0.97       718\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       908\n",
      "   macro avg       0.92      0.94      0.93       908\n",
      "weighted avg       0.95      0.95      0.95       908\n",
      "\n",
      "0.9251748452189865\n",
      "[0.9243     0.8923     0.8661     0.8326     0.9255     0.8957\n",
      " 0.8901     0.9463     0.8334     0.905      0.9016     0.871\n",
      " 0.8871     0.8875     0.8737     0.9155     0.9252     0.89252369]\n"
     ]
    }
   ],
   "source": [
    "#self study\n",
    "posCommon = {}\n",
    "negCommon = {}\n",
    "\n",
    "scores = np.zeros(21-startID)\n",
    "k = 0\n",
    "f1Score = 0\n",
    "\n",
    "#initial learning\n",
    "for file in files[startID:21]:\n",
    "    trainData = pd.read_csv(file,sep='\\t')\n",
    "\n",
    "    ratio = 0.2\n",
    "\n",
    "    dataRow = trainData.shape[0]\n",
    "    print('******************************************************************')\n",
    "    print('ratio is: ',ratio)\n",
    "    print('Processing ',file,'with ',dataRow,' rows.')\n",
    "    X_train = trainData['Review']\n",
    "    \n",
    "    y_train = [calculateProba(review) for review in X_train]\n",
    "    y_train = ['POS' if tem>0.5 else 'NEG' if tem<-0.5 else 'NEU' for tem in y_train]\n",
    "\n",
    "    #print('Shape of trainData: ',trainData.shape)\n",
    "    #print('Shape of testData: ',testData.shape)\n",
    "\n",
    "    buffer =  trainData.apply(lambda X: countFrequence(X),axis =1)\n",
    "    #print('TrainData counting completed')\n",
    "\n",
    "    posVoca = list(posFreq.keys())\n",
    "    negVoca = list(negFreq.keys())\n",
    "    vocabulary = set(posVoca + negVoca)\n",
    "\n",
    "    posTotalFreq = 0\n",
    "    for word in posVoca:\n",
    "        posTotalFreq += posFreq.get(word)\n",
    "\n",
    "    negTotalFreq = 0\n",
    "    for word in negVoca:\n",
    "        negTotalFreq += negFreq.get(word)\n",
    "\n",
    "    lambdaSmoothing = 1\n",
    "\n",
    "    posProb = {}\n",
    "\n",
    "    for word in vocabulary:\n",
    "        frequence = posFreq.get(word)\n",
    "        if frequence is None:\n",
    "            frequence = 0\n",
    "\n",
    "        posProb.update({word: (lambdaSmoothing + frequence)/(lambdaSmoothing*len(vocabulary) + posTotalFreq)})\n",
    "\n",
    "    negProb = {}\n",
    "    wordProb = {}\n",
    "\n",
    "    for word in vocabulary:\n",
    "        frequence = negFreq.get(word)\n",
    "        if frequence is None:\n",
    "            frequence = 0\n",
    "\n",
    "        negProb.update({word: (lambdaSmoothing + frequence)/(lambdaSmoothing*len(vocabulary) + negTotalFreq)})\n",
    "\n",
    "    totalWords = sum(posFreq.values()) + sum(negFreq.values())\n",
    "\n",
    "    for word in vocabulary:\n",
    "        try: \n",
    "            frequence = posFreq.get(word) + 0\n",
    "            try:\n",
    "                frequence += negFreq.get(word)\n",
    "            except:\n",
    "                pass\n",
    "        except:\n",
    "            frequence = negFreq.get(word)\n",
    "        wordProb.update({word:frequence/totalWords})\n",
    "\n",
    "\n",
    "    ## To calculate the orientation of words\n",
    "    orientation = {}\n",
    "    for word in vocabulary:\n",
    "        if wordFreq.get(word) > 5*(k+startID): #只出现一次的词偶然性太大\n",
    "            orientation.update({word:posProb.get(word)/wordProb.get(word)})\n",
    "        else:\n",
    "            orientation.update({word:1})\n",
    "\n",
    "\n",
    "    #sorted() small to large, smaller means more negative \n",
    "    threshold1 = (sorted(orientation.values()))[math.ceil(len(orientation)*ratio/2)]\n",
    "    threshold2 = (sorted(orientation.values()))[-math.ceil(len(orientation)*ratio/2)]\n",
    "    #print('threshold is: ', threshold)\n",
    "    #totalThreshold += threshold\n",
    "\n",
    "    #Only use the words whose orientation larger than threshold\n",
    "    for word in posProb:\n",
    "        if (orientation.get(word) > threshold1) and  (orientation.get(word) < threshold2):\n",
    "            posProb.update({word: (lambdaSmoothing + 0)/(lambdaSmoothing*len(vocabulary) + posTotalFreq)})\n",
    "\n",
    "    for word in negFreq:\n",
    "        if (orientation.get(word) > threshold1) and  (orientation.get(word) < threshold2):\n",
    "            negProb.update({word: (lambdaSmoothing + 0)/(lambdaSmoothing*len(vocabulary) + negTotalFreq)})\n",
    "\n",
    "    negValues = np.array((sorted(orientation.values())))[:math.ceil(len(orientation)*ratio/2)]\n",
    "    posValues  = np.array((sorted(orientation.values())))[-math.ceil(len(orientation)*ratio/2):]\n",
    "\n",
    "    for word in vocabulary:\n",
    "        if (orientation.get(word) > threshold2):\n",
    "            try:\n",
    "                weight = 1 + np.where(posValues==orientation.get(word))[0][0]/len(posValues)\n",
    "                posCommon.update({word:posCommon.get(word)+weight})\n",
    "            except:\n",
    "                posCommon.update({word:1})\n",
    "        elif (orientation.get(word) < threshold1):\n",
    "            try:\n",
    "                weight = 1 + -(np.where(negValues==orientation.get(word))[0][0]-len(posValues))/len(posValues)\n",
    "                negCommon.update({word:negCommon.get(word)+weight})\n",
    "            except:\n",
    "                negCommon.update({word:1})\n",
    "\n",
    "    posNum = sum(trainData['Label'] == 'POS')\n",
    "    negNum = sum(trainData['Label'] == 'NEG')\n",
    "    posPriorProb = posNum/(posNum+negNum)\n",
    "    negPriorProb = negNum/(posNum+negNum)\n",
    "    \n",
    "    testData = pd.read_csv(file,sep='\\t')\n",
    "    testData = testData[testData['Label'] != 'NEU']\n",
    "    print('******************************************************************')\n",
    "    print('Processing ',file,'with ',dataRow,' rows.')\n",
    "    predict = [calculateProba(review) for review in testData['Review']]\n",
    "    result = np.zeros([len(predict),2])\n",
    "    result = pd.DataFrame(result)\n",
    "    result[0]=[1 if tem>0 else 0 for tem in predict]\n",
    "    result[1]=list(testData['Label'])\n",
    "    result[1] = [ 1 if (tem == 'POS') else 0 for tem in list(result[1])]\n",
    "    target_names = ['class 0', 'class 1']\n",
    "    print(classification_report(result[1], result[0], target_names=target_names))\n",
    "    print(sklearn.metrics.f1_score(result[1],result[0], average='macro'))\n",
    "    scores[k] = round(sklearn.metrics.f1_score(result[1],result[0], average='macro'),4)\n",
    "    \n",
    "    f1Score += sklearn.metrics.f1_score(result[1],result[0], average='macro')\n",
    "    k += 1\n",
    "scores[-1]=f1Score/k\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8694    , 0.8748    , 0.8259    , 0.785     , 0.9121    ,\n",
       "       0.8768    , 0.8858    , 0.9236    , 0.7599    , 0.8707    ,\n",
       "       0.8932    , 0.8381    , 0.8575    , 0.8475    , 0.8059    ,\n",
       "       0.8992    , 0.9068    , 0.86071391])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=sorted(negCommon.items(), key = lambda x: x[1], reverse = True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refund  &  32.99921813917123 \\\\ \n",
      "garbage  &  32.994266353922335 \\\\ \n",
      "junk  &  32.985405264529575 \\\\ \n",
      "waste  &  32.984102163148286 \\\\ \n",
      "worst  &  32.97185301016418 \\\\ \n",
      "rma  &  32.96846494657285 \\\\ \n",
      "poorly  &  32.96194943966641 \\\\ \n",
      "terrible  &  32.95569455303623 \\\\ \n",
      "disappointed  &  32.949960906958566 \\\\ \n",
      "trash  &  32.948918425853535 \\\\ \n",
      "useless  &  32.94683346364347 \\\\ \n",
      "worthless  &  32.94057857701329 \\\\ \n",
      "awful  &  32.92520198071411 \\\\ \n",
      "defective  &  32.917904612978894 \\\\ \n",
      "return  &  32.913734688558776 \\\\ \n",
      "exchange  &  32.908001042481104 \\\\ \n",
      "respond  &  32.90487359916601 \\\\ \n",
      "poor  &  32.90409173833724 \\\\ \n",
      "disappointment  &  32.90278863695596 \\\\ \n",
      "crap  &  32.89653375032577 \\\\ \n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    print(i[0],' & ',i[1],'\\\\\\ ')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "startID=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************\n",
      "Processing  AlarmClock.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.80      0.91      0.85       274\n",
      "     class 1       0.96      0.90      0.93       624\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       898\n",
      "   macro avg       0.88      0.90      0.89       898\n",
      "weighted avg       0.91      0.90      0.90       898\n",
      "\n",
      "******************************************************************\n",
      "Processing  Baby.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.87      0.63      0.73       150\n",
      "     class 1       0.93      0.98      0.96       762\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       912\n",
      "   macro avg       0.90      0.80      0.84       912\n",
      "weighted avg       0.92      0.92      0.92       912\n",
      "\n",
      "******************************************************************\n",
      "Processing  Bag.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.80      0.60      0.68       110\n",
      "     class 1       0.95      0.98      0.96       809\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       919\n",
      "   macro avg       0.87      0.79      0.82       919\n",
      "weighted avg       0.93      0.93      0.93       919\n",
      "\n",
      "******************************************************************\n",
      "Processing  CableModem.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.64      0.93      0.76       121\n",
      "     class 1       0.99      0.93      0.96       845\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       966\n",
      "   macro avg       0.82      0.93      0.86       966\n",
      "weighted avg       0.95      0.93      0.93       966\n",
      "\n",
      "******************************************************************\n",
      "Processing  Dumbbell.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.79      0.80      0.79       146\n",
      "     class 1       0.96      0.96      0.96       764\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       910\n",
      "   macro avg       0.87      0.88      0.88       910\n",
      "weighted avg       0.93      0.93      0.93       910\n",
      "\n",
      "******************************************************************\n",
      "Processing  Flashlight.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.75      0.78      0.76       108\n",
      "     class 1       0.97      0.97      0.97       816\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       924\n",
      "   macro avg       0.86      0.87      0.87       924\n",
      "weighted avg       0.94      0.94      0.94       924\n",
      "\n",
      "******************************************************************\n",
      "Processing  Gloves.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.82      0.61      0.70       127\n",
      "     class 1       0.94      0.98      0.96       796\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       923\n",
      "   macro avg       0.88      0.79      0.83       923\n",
      "weighted avg       0.92      0.93      0.92       923\n",
      "\n",
      "******************************************************************\n",
      "Processing  GPS.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.77      0.94      0.85       179\n",
      "     class 1       0.98      0.93      0.96       739\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       918\n",
      "   macro avg       0.88      0.94      0.90       918\n",
      "weighted avg       0.94      0.93      0.94       918\n",
      "\n",
      "******************************************************************\n",
      "Processing  GraphicsCard.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.83      0.80      0.82       136\n",
      "     class 1       0.97      0.97      0.97       797\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       933\n",
      "   macro avg       0.90      0.89      0.89       933\n",
      "weighted avg       0.95      0.95      0.95       933\n",
      "\n",
      "******************************************************************\n",
      "Processing  Headphone.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.80      0.82      0.81       187\n",
      "     class 1       0.95      0.94      0.95       704\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       891\n",
      "   macro avg       0.87      0.88      0.88       891\n",
      "weighted avg       0.92      0.92      0.92       891\n",
      "\n",
      "******************************************************************\n",
      "Processing  HomeTheaterSystem.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.92      0.92      0.92       261\n",
      "     class 1       0.97      0.97      0.97       644\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       905\n",
      "   macro avg       0.94      0.94      0.94       905\n",
      "weighted avg       0.95      0.95      0.95       905\n",
      "\n",
      "******************************************************************\n",
      "Processing  Jewelry.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.81      0.67      0.74       110\n",
      "     class 1       0.96      0.98      0.97       791\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       901\n",
      "   macro avg       0.88      0.83      0.85       901\n",
      "weighted avg       0.94      0.94      0.94       901\n",
      "\n",
      "******************************************************************\n",
      "Processing  Keyboard.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.87      0.81      0.84       203\n",
      "     class 1       0.94      0.97      0.96       693\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       896\n",
      "   macro avg       0.91      0.89      0.90       896\n",
      "weighted avg       0.93      0.93      0.93       896\n",
      "\n",
      "******************************************************************\n",
      "Processing  Magazine_Subscriptions.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.78      0.97      0.86       247\n",
      "     class 1       0.99      0.90      0.94       672\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       919\n",
      "   macro avg       0.88      0.93      0.90       919\n",
      "weighted avg       0.93      0.92      0.92       919\n",
      "\n",
      "******************************************************************\n",
      "Processing  Movies_TV.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.81      0.77      0.79       101\n",
      "     class 1       0.97      0.98      0.98       829\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       930\n",
      "   macro avg       0.89      0.88      0.88       930\n",
      "weighted avg       0.96      0.96      0.96       930\n",
      "\n",
      "******************************************************************\n",
      "Processing  Projector.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.86      0.78      0.82       186\n",
      "     class 1       0.95      0.97      0.96       733\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       919\n",
      "   macro avg       0.90      0.87      0.89       919\n",
      "weighted avg       0.93      0.93      0.93       919\n",
      "\n",
      "******************************************************************\n",
      "Processing  RiceCooker.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.91      0.75      0.83       175\n",
      "     class 1       0.95      0.98      0.96       764\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       939\n",
      "   macro avg       0.93      0.87      0.89       939\n",
      "weighted avg       0.94      0.94      0.94       939\n",
      "\n",
      "******************************************************************\n",
      "Processing  Sandal.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.90      0.69      0.78       115\n",
      "     class 1       0.96      0.99      0.97       835\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       950\n",
      "   macro avg       0.93      0.84      0.88       950\n",
      "weighted avg       0.95      0.95      0.95       950\n",
      "\n",
      "******************************************************************\n",
      "Processing  Vacuum.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.88      0.86      0.87       203\n",
      "     class 1       0.96      0.97      0.96       717\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       920\n",
      "   macro avg       0.92      0.91      0.92       920\n",
      "weighted avg       0.94      0.94      0.94       920\n",
      "\n",
      "******************************************************************\n",
      "Processing  Video_Games.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.86      0.91      0.88       190\n",
      "     class 1       0.98      0.96      0.97       718\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       908\n",
      "   macro avg       0.92      0.94      0.93       908\n",
      "weighted avg       0.95      0.95      0.95       908\n",
      "\n",
      "[0.8874     0.842      0.8234     0.8588     0.8766     0.8658\n",
      " 0.8278     0.902      0.8929     0.8777     0.9433     0.8516\n",
      " 0.897      0.9006     0.8836     0.8865     0.8945     0.8759\n",
      " 0.9155     0.9252     0.88140842]\n"
     ]
    }
   ],
   "source": [
    "scores1 = np.zeros(21-startID)\n",
    "k = 0\n",
    "f1Score = 0\n",
    "for file in files[startID:21]:\n",
    "    testData = pd.read_csv(file,sep='\\t')\n",
    "    testData = testData[testData['Label'] != 'NEU']\n",
    "    print('******************************************************************')\n",
    "    print('Processing ',file,'with ',dataRow,' rows.')\n",
    "    predict = [calculateProba(review) for review in testData['Review']]\n",
    "    result = np.zeros([len(predict),2])\n",
    "    result = pd.DataFrame(result)\n",
    "    result[0]=[1 if tem>0 else 0 for tem in predict]\n",
    "    result[1]=list(testData['Label'])\n",
    "    result[1] = [ 1 if (tem == 'POS') else 0 for tem in list(result[1])]\n",
    "    target_names = ['class 0', 'class 1']\n",
    "    print(classification_report(result[1], result[0], target_names=target_names))\n",
    "    scores1[k] = round(sklearn.metrics.f1_score(result[1],result[0], average='macro'),4)\n",
    "    \n",
    "    f1Score += sklearn.metrics.f1_score(result[1],result[0], average='macro')\n",
    "    k += 1\n",
    "scores1[-1]=f1Score/k\n",
    "print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sa', 'bd']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F1.jpg'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array([0.8694    , 0.8748    , 0.8259    , 0.785     , 0.9121    ,\n",
    "       0.8768    , 0.8858    , 0.9236    , 0.7599    , 0.8707    ,\n",
    "       0.8932    , 0.8381    , 0.8575    , 0.8475    , 0.8059    ,\n",
    "       0.8992    , 0.9068    , 0.86071391])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9243    , 0.8923    , 0.8661    , 0.8326    , 0.9255    ,\n",
       "       0.8957    , 0.8901    , 0.9463    , 0.8334    , 0.905     ,\n",
       "       0.9016    , 0.871     , 0.8871    , 0.8875    , 0.8737    ,\n",
       "       0.9155    , 0.9252    , 0.89252369])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8923, 0.8326])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[[1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    do a good job , the LED 's let you know when t...\n",
       "3    DaGeDar be a game where you control a DaGeDar ...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData['Review'].iloc[[1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domain</th>\n",
       "      <th>Label</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Video_Games</td>\n",
       "      <td>NEU</td>\n",
       "      <td>3</td>\n",
       "      <td>this game feature a new 3d setup , which with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Video_Games</td>\n",
       "      <td>POS</td>\n",
       "      <td>5</td>\n",
       "      <td>do a good job , the LED 's let you know when t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Video_Games</td>\n",
       "      <td>NEU</td>\n",
       "      <td>3</td>\n",
       "      <td>this product be definately not all it be crack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Video_Games</td>\n",
       "      <td>NEU</td>\n",
       "      <td>3</td>\n",
       "      <td>DaGeDar be a game where you control a DaGeDar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Video_Games</td>\n",
       "      <td>POS</td>\n",
       "      <td>4</td>\n",
       "      <td>I get this game on the first day . I be so exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Video_Games</td>\n",
       "      <td>POS</td>\n",
       "      <td>5</td>\n",
       "      <td>my wife and she sister play these game . they ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Video_Games</td>\n",
       "      <td>POS</td>\n",
       "      <td>5</td>\n",
       "      <td>I purchase this remote because I intend to use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Video_Games</td>\n",
       "      <td>NEG</td>\n",
       "      <td>1</td>\n",
       "      <td>not really a lot to say here . the graphic be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Video_Games</td>\n",
       "      <td>POS</td>\n",
       "      <td>5</td>\n",
       "      <td>ok before I start on the game , I will address...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Video_Games</td>\n",
       "      <td>NEG</td>\n",
       "      <td>1</td>\n",
       "      <td>xbox 360 mc2 race wheel be not work for I : -l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Domain Label  Rating  \\\n",
       "0  Video_Games   NEU       3   \n",
       "1  Video_Games   POS       5   \n",
       "2  Video_Games   NEU       3   \n",
       "3  Video_Games   NEU       3   \n",
       "4  Video_Games   POS       4   \n",
       "5  Video_Games   POS       5   \n",
       "6  Video_Games   POS       5   \n",
       "7  Video_Games   NEG       1   \n",
       "8  Video_Games   POS       5   \n",
       "9  Video_Games   NEG       1   \n",
       "\n",
       "                                              Review  \n",
       "0  this game feature a new 3d setup , which with ...  \n",
       "1  do a good job , the LED 's let you know when t...  \n",
       "2  this product be definately not all it be crack...  \n",
       "3  DaGeDar be a game where you control a DaGeDar ...  \n",
       "4  I get this game on the first day . I be so exc...  \n",
       "5  my wife and she sister play these game . they ...  \n",
       "6  I purchase this remote because I intend to use...  \n",
       "7  not really a lot to say here . the graphic be ...  \n",
       "8  ok before I start on the game , I will address...  \n",
       "9  xbox 360 mc2 race wheel be not work for I : -l...  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.iloc[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
