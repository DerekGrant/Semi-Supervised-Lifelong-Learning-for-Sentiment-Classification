{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from string import digits\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "import string\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_name(file_dir): \n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        print(root) #当前目录路径\n",
    "        print(dirs) #当前路径下所有子目录\n",
    "        print(files) #当前路径下所有非目录子文件\n",
    "        return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\data\n",
      "[]\n",
      "['AlarmClock.txt', 'Baby.txt', 'Bag.txt', 'CableModem.txt', 'Dumbbell.txt', 'F1.jpg', 'F1.png', 'Flashlight.txt', 'Gloves.txt', 'GPS.txt', 'GraphicsCard.txt', 'Headphone.txt', 'HomeTheaterSystem.txt', 'Jewelry.txt', 'Keyboard.txt', 'Magazine_Subscriptions.txt', 'Movies_TV.txt', 'Projector.txt', 'RiceCooker.txt', 'Sandal.txt', 'Vacuum.txt', 'Video_Games.txt']\n"
     ]
    }
   ],
   "source": [
    "files = file_name('..\\data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanLines(line):\n",
    "    return line.lower() #换小写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countFrequence(raw):\n",
    "    review = raw['Review']\n",
    "    label = raw['Label']\n",
    "    \n",
    "    words = CleanLines(review).split()\n",
    "                \n",
    "    for word in words:\n",
    "        try:\n",
    "            wordFreq.update({word:wordFreq.get(word)+1})\n",
    "        except:\n",
    "            wordFreq.update({word:1})\n",
    "                \n",
    "        if label == 'POS':\n",
    "            try:\n",
    "                posFreq.update({word:posFreq.get(word)+1})\n",
    "            except:\n",
    "                posFreq.update({word:1})\n",
    "        elif label == 'NEG':\n",
    "            try:\n",
    "                negFreq.update({word:negFreq.get(word)+1})\n",
    "            except:\n",
    "                negFreq.update({word:1})                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateProba(review):\n",
    "    words = CleanLines(review).split()\n",
    "    \n",
    "    posProba = math.log10(posPriorProb)\n",
    "    negProba = math.log10(negPriorProb)\n",
    "    for word in words:\n",
    "        wordPosProba = posProb.get(word)\n",
    "        if wordPosProba is None:\n",
    "            posProba += math.log10( (lambdaSmoothing + 0)/(lambdaSmoothing*len(vocabulary) + posTotalFreq)) ##\n",
    "        else:\n",
    "            posProba += math.log10(wordPosProba)\n",
    "        \n",
    "        \n",
    "        wordNegProba = negProb.get(word)\n",
    "        if wordNegProba is None:\n",
    "            negProba += math.log10((lambdaSmoothing + 0)/(lambdaSmoothing*len(vocabulary) + negTotalFreq))\n",
    "        else:\n",
    "            negProba += math.log10(wordNegProba)\n",
    "    try:\n",
    "        return (1-10**(negProba-posProba))/(1+10**(negProba-posProba)) ##normalize to -1,1\n",
    "    except:\n",
    "        if(negProba-posProba>20):\n",
    "            return (1-10**int(negProba-posProba))/(1+10**int(negProba-posProba)) ##normalize to -1,1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Bag.txt with  3000  rows.\n"
     ]
    }
   ],
   "source": [
    "posCommon = {}\n",
    "negCommon = {}\n",
    "startID = 3\n",
    "file = files[0]\n",
    "trainData = pd.read_csv(file,sep='\\t')\n",
    "\n",
    "#initial learning\n",
    "for file in files[1:startID]:\n",
    "    trainData = pd.concat([trainData,pd.read_csv(file,sep='\\t')])\n",
    "\n",
    "    \n",
    "ratio = 0.2\n",
    "\n",
    "dataRow = trainData.shape[0]\n",
    "print('******************************************************************')\n",
    "print('ratio is: ',ratio)\n",
    "print('Processing ',file,'with ',dataRow,' rows.')\n",
    "X_train = trainData['Review']\n",
    "y_train = trainData['Label']\n",
    "\n",
    "\n",
    "#print('Shape of trainData: ',trainData.shape)\n",
    "#print('Shape of testData: ',testData.shape)\n",
    "\n",
    "wordFreq = {}\n",
    "posFreq = {}\n",
    "negFreq = {}\n",
    "buffer =  trainData.apply(lambda X: countFrequence(X),axis =1)\n",
    "#print('TrainData counting completed')\n",
    "\n",
    "posVoca = list(posFreq.keys())\n",
    "negVoca = list(negFreq.keys())\n",
    "vocabulary = set(posVoca + negVoca)\n",
    "\n",
    "posTotalFreq = 0\n",
    "for word in posVoca:\n",
    "    posTotalFreq += posFreq.get(word)\n",
    "\n",
    "negTotalFreq = 0\n",
    "for word in negVoca:\n",
    "    negTotalFreq += negFreq.get(word)\n",
    "\n",
    "lambdaSmoothing = 1\n",
    "\n",
    "posProb = {}\n",
    "\n",
    "for word in vocabulary:\n",
    "    frequence = posFreq.get(word)\n",
    "    if frequence is None:\n",
    "        frequence = 0\n",
    "\n",
    "    posProb.update({word: (lambdaSmoothing + frequence)/(lambdaSmoothing*len(vocabulary) + posTotalFreq)})\n",
    "\n",
    "negProb = {}\n",
    "wordProb = {}\n",
    "\n",
    "for word in vocabulary:\n",
    "    frequence = negFreq.get(word)\n",
    "    if frequence is None:\n",
    "        frequence = 0\n",
    "\n",
    "    negProb.update({word: (lambdaSmoothing + frequence)/(lambdaSmoothing*len(vocabulary) + negTotalFreq)})\n",
    "\n",
    "totalWords = sum(posFreq.values()) + sum(negFreq.values())\n",
    "\n",
    "for word in vocabulary:\n",
    "    try: \n",
    "        frequence = posFreq.get(word) + 0\n",
    "        try:\n",
    "            frequence += negFreq.get(word)\n",
    "        except:\n",
    "            pass\n",
    "    except:\n",
    "        frequence = negFreq.get(word)\n",
    "    wordProb.update({word:frequence/totalWords})\n",
    "\n",
    "\n",
    "## To calculate the orientation of words\n",
    "orientation = {}\n",
    "for word in vocabulary:\n",
    "    if wordFreq.get(word) > 5*startID: #只出现一次的词偶然性太大\n",
    "        orientation.update({word:posProb.get(word)/wordProb.get(word)})\n",
    "    else:\n",
    "        orientation.update({word:1})\n",
    "\n",
    "\n",
    "#sorted() small to large, smaller means more negative \n",
    "threshold1 = (sorted(orientation.values()))[math.ceil(len(orientation)*ratio/2)]\n",
    "threshold2 = (sorted(orientation.values()))[-math.ceil(len(orientation)*ratio/2)]\n",
    "#print('threshold is: ', threshold)\n",
    "#totalThreshold += threshold\n",
    "\n",
    "#Only use the words whose orientation larger than threshold\n",
    "for word in posProb:\n",
    "    if (orientation.get(word) > threshold1) and  (orientation.get(word) < threshold2):\n",
    "        posProb.update({word: (lambdaSmoothing + 0)/(lambdaSmoothing*len(vocabulary) + posTotalFreq)})\n",
    "\n",
    "for word in negFreq:\n",
    "    if (orientation.get(word) > threshold1) and  (orientation.get(word) < threshold2):\n",
    "        negProb.update({word: (lambdaSmoothing + 0)/(lambdaSmoothing*len(vocabulary) + negTotalFreq)})\n",
    "\n",
    "negValues = np.array((sorted(orientation.values())))[:math.ceil(len(orientation)*ratio/2)]\n",
    "posValues  = np.array((sorted(orientation.values())))[-math.ceil(len(orientation)*ratio/2):]\n",
    "\n",
    "for word in vocabulary:\n",
    "    if (orientation.get(word) > threshold2):\n",
    "        try:\n",
    "            weight = 1 + np.where(posValues==orientation.get(word))[0][0]/len(posValues)\n",
    "            posCommon.update({word:posCommon.get(word)+weight})\n",
    "        except:\n",
    "            posCommon.update({word:1})\n",
    "    elif (orientation.get(word) < threshold1):\n",
    "        try:\n",
    "            weight = 1 + -(np.where(negValues==orientation.get(word))[0][0]-len(posValues))/len(posValues)\n",
    "            negCommon.update({word:negCommon.get(word)+weight})\n",
    "        except:\n",
    "            negCommon.update({word:1})\n",
    "\n",
    "posNum = sum(trainData['Label'] == 'POS')\n",
    "negNum = sum(trainData['Label'] == 'NEG')\n",
    "posPriorProb = posNum/(posNum+negNum)\n",
    "negPriorProb = negNum/(posNum+negNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  CableModem.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  CableModem.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.94      0.80      0.87       121\n",
      "     class 1       0.97      0.99      0.98       845\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       966\n",
      "   macro avg       0.96      0.90      0.92       966\n",
      "weighted avg       0.97      0.97      0.97       966\n",
      "\n",
      "0.924253512880562\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Dumbbell.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Dumbbell.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.93      0.73      0.82       146\n",
      "     class 1       0.95      0.99      0.97       764\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       910\n",
      "   macro avg       0.94      0.86      0.89       910\n",
      "weighted avg       0.95      0.95      0.94       910\n",
      "\n",
      "0.8923076923076922\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Flashlight.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Flashlight.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.90      0.66      0.76       108\n",
      "     class 1       0.96      0.99      0.97       816\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       924\n",
      "   macro avg       0.93      0.82      0.87       924\n",
      "weighted avg       0.95      0.95      0.95       924\n",
      "\n",
      "0.8661330877926126\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Gloves.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Gloves.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.92      0.57      0.70       127\n",
      "     class 1       0.93      0.99      0.96       796\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       923\n",
      "   macro avg       0.93      0.78      0.83       923\n",
      "weighted avg       0.93      0.93      0.93       923\n",
      "\n",
      "0.8326332842853108\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  GPS.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  GPS.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.85      0.91      0.88       179\n",
      "     class 1       0.98      0.96      0.97       739\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       918\n",
      "   macro avg       0.92      0.94      0.93       918\n",
      "weighted avg       0.95      0.95      0.95       918\n",
      "\n",
      "0.9255337192581394\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  GraphicsCard.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  GraphicsCard.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.90      0.75      0.82       136\n",
      "     class 1       0.96      0.99      0.97       797\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       933\n",
      "   macro avg       0.93      0.87      0.90       933\n",
      "weighted avg       0.95      0.95      0.95       933\n",
      "\n",
      "0.8957238974450678\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Headphone.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Headphone.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.86      0.79      0.82       187\n",
      "     class 1       0.95      0.97      0.96       704\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       891\n",
      "   macro avg       0.90      0.88      0.89       891\n",
      "weighted avg       0.93      0.93      0.93       891\n",
      "\n",
      "0.8901199357158658\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  HomeTheaterSystem.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  HomeTheaterSystem.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.92      0.93      0.92       261\n",
      "     class 1       0.97      0.97      0.97       644\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       905\n",
      "   macro avg       0.95      0.95      0.95       905\n",
      "weighted avg       0.96      0.96      0.96       905\n",
      "\n",
      "0.9462799615352653\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Jewelry.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Jewelry.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.90      0.57      0.70       110\n",
      "     class 1       0.94      0.99      0.97       791\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       901\n",
      "   macro avg       0.92      0.78      0.83       901\n",
      "weighted avg       0.94      0.94      0.93       901\n",
      "\n",
      "0.8333538840937115\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Keyboard.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Keyboard.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.91      0.80      0.85       203\n",
      "     class 1       0.94      0.98      0.96       693\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       896\n",
      "   macro avg       0.93      0.89      0.90       896\n",
      "weighted avg       0.94      0.94      0.93       896\n",
      "\n",
      "0.9049984095715888\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Magazine_Subscriptions.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Magazine_Subscriptions.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.78      0.96      0.86       247\n",
      "     class 1       0.99      0.90      0.94       672\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       919\n",
      "   macro avg       0.88      0.93      0.90       919\n",
      "weighted avg       0.93      0.92      0.92       919\n",
      "\n",
      "0.9016104311763247\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Movies_TV.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Movies_TV.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.82      0.72      0.77       101\n",
      "     class 1       0.97      0.98      0.97       829\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       930\n",
      "   macro avg       0.89      0.85      0.87       930\n",
      "weighted avg       0.95      0.95      0.95       930\n",
      "\n",
      "0.8710368736211787\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Projector.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Projector.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.87      0.77      0.82       186\n",
      "     class 1       0.94      0.97      0.96       733\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       919\n",
      "   macro avg       0.91      0.87      0.89       919\n",
      "weighted avg       0.93      0.93      0.93       919\n",
      "\n",
      "0.8870660522273426\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  RiceCooker.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  RiceCooker.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.91      0.73      0.81       175\n",
      "     class 1       0.94      0.98      0.96       764\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       939\n",
      "   macro avg       0.93      0.86      0.89       939\n",
      "weighted avg       0.94      0.94      0.93       939\n",
      "\n",
      "0.8874752460165127\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Sandal.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Sandal.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      0.65      0.77       115\n",
      "     class 1       0.95      1.00      0.97       835\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       950\n",
      "   macro avg       0.95      0.82      0.87       950\n",
      "weighted avg       0.95      0.95      0.95       950\n",
      "\n",
      "0.8737022757762174\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Vacuum.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Vacuum.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.88      0.86      0.87       203\n",
      "     class 1       0.96      0.97      0.96       717\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       920\n",
      "   macro avg       0.92      0.91      0.92       920\n",
      "weighted avg       0.94      0.94      0.94       920\n",
      "\n",
      "0.9154996456045432\n",
      "******************************************************************\n",
      "ratio is:  0.2\n",
      "Processing  Video_Games.txt with  1000  rows.\n",
      "******************************************************************\n",
      "Processing  Video_Games.txt with  1000  rows.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.86      0.91      0.88       190\n",
      "     class 1       0.98      0.96      0.97       718\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       908\n",
      "   macro avg       0.92      0.94      0.93       908\n",
      "weighted avg       0.95      0.95      0.95       908\n",
      "\n",
      "0.9251748452189865\n",
      "[0.9243     0.8923     0.8661     0.8326     0.9255     0.8957\n",
      " 0.8901     0.9463     0.8334     0.905      0.9016     0.871\n",
      " 0.8871     0.8875     0.8737     0.9155     0.9252     0.89252369]\n"
     ]
    }
   ],
   "source": [
    "posCommon = {}\n",
    "negCommon = {}\n",
    "\n",
    "scores = np.zeros(21-startID)\n",
    "k = 0\n",
    "f1Score = 0\n",
    "\n",
    "#initial learning\n",
    "for file in files[startID:21]:\n",
    "    trainData = pd.read_csv(file,sep='\\t')\n",
    "\n",
    "    ratio = 0.2\n",
    "\n",
    "    dataRow = trainData.shape[0]\n",
    "    print('******************************************************************')\n",
    "    print('ratio is: ',ratio)\n",
    "    print('Processing ',file,'with ',dataRow,' rows.')\n",
    "    X_train = trainData['Review']\n",
    "    \n",
    "    y_train = [calculateProba(review) for review in X_train]\n",
    "    y_train = ['POS' if tem>0.5 else 'NEG' if tem<-0.5 else 'NEU' for tem in y_train]\n",
    "\n",
    "    #print('Shape of trainData: ',trainData.shape)\n",
    "    #print('Shape of testData: ',testData.shape)\n",
    "\n",
    "    buffer =  trainData.apply(lambda X: countFrequence(X),axis =1)\n",
    "    #print('TrainData counting completed')\n",
    "\n",
    "    posVoca = list(posFreq.keys())\n",
    "    negVoca = list(negFreq.keys())\n",
    "    vocabulary = set(posVoca + negVoca)\n",
    "\n",
    "    posTotalFreq = 0\n",
    "    for word in posVoca:\n",
    "        posTotalFreq += posFreq.get(word)\n",
    "\n",
    "    negTotalFreq = 0\n",
    "    for word in negVoca:\n",
    "        negTotalFreq += negFreq.get(word)\n",
    "\n",
    "    lambdaSmoothing = 1\n",
    "\n",
    "    posProb = {}\n",
    "\n",
    "    for word in vocabulary:\n",
    "        frequence = posFreq.get(word)\n",
    "        if frequence is None:\n",
    "            frequence = 0\n",
    "\n",
    "        posProb.update({word: (lambdaSmoothing + frequence)/(lambdaSmoothing*len(vocabulary) + posTotalFreq)})\n",
    "\n",
    "    negProb = {}\n",
    "    wordProb = {}\n",
    "\n",
    "    for word in vocabulary:\n",
    "        frequence = negFreq.get(word)\n",
    "        if frequence is None:\n",
    "            frequence = 0\n",
    "\n",
    "        negProb.update({word: (lambdaSmoothing + frequence)/(lambdaSmoothing*len(vocabulary) + negTotalFreq)})\n",
    "\n",
    "    totalWords = sum(posFreq.values()) + sum(negFreq.values())\n",
    "\n",
    "    for word in vocabulary:\n",
    "        try: \n",
    "            frequence = posFreq.get(word) + 0\n",
    "            try:\n",
    "                frequence += negFreq.get(word)\n",
    "            except:\n",
    "                pass\n",
    "        except:\n",
    "            frequence = negFreq.get(word)\n",
    "        wordProb.update({word:frequence/totalWords})\n",
    "\n",
    "\n",
    "    ## To calculate the orientation of words\n",
    "    orientation = {}\n",
    "    for word in vocabulary:\n",
    "        if wordFreq.get(word) > 5*(k+startID): #只出现一次的词偶然性太大\n",
    "            orientation.update({word:posProb.get(word)/wordProb.get(word)})\n",
    "        else:\n",
    "            orientation.update({word:1})\n",
    "\n",
    "\n",
    "    #sorted() small to large, smaller means more negative \n",
    "    threshold1 = (sorted(orientation.values()))[math.ceil(len(orientation)*ratio/2)]\n",
    "    threshold2 = (sorted(orientation.values()))[-math.ceil(len(orientation)*ratio/2)]\n",
    "    #print('threshold is: ', threshold)\n",
    "    #totalThreshold += threshold\n",
    "\n",
    "    #Only use the words whose orientation larger than threshold\n",
    "    for word in posProb:\n",
    "        if (orientation.get(word) > threshold1) and  (orientation.get(word) < threshold2):\n",
    "            posProb.update({word: (lambdaSmoothing + 0)/(lambdaSmoothing*len(vocabulary) + posTotalFreq)})\n",
    "\n",
    "    for word in negFreq:\n",
    "        if (orientation.get(word) > threshold1) and  (orientation.get(word) < threshold2):\n",
    "            negProb.update({word: (lambdaSmoothing + 0)/(lambdaSmoothing*len(vocabulary) + negTotalFreq)})\n",
    "\n",
    "    negValues = np.array((sorted(orientation.values())))[:math.ceil(len(orientation)*ratio/2)]\n",
    "    posValues  = np.array((sorted(orientation.values())))[-math.ceil(len(orientation)*ratio/2):]\n",
    "\n",
    "    for word in vocabulary:\n",
    "        if (orientation.get(word) > threshold2):\n",
    "            try:\n",
    "                weight = 1 + np.where(posValues==orientation.get(word))[0][0]/len(posValues)\n",
    "                posCommon.update({word:posCommon.get(word)+weight})\n",
    "            except:\n",
    "                posCommon.update({word:1})\n",
    "        elif (orientation.get(word) < threshold1):\n",
    "            try:\n",
    "                weight = 1 + -(np.where(negValues==orientation.get(word))[0][0]-len(posValues))/len(posValues)\n",
    "                negCommon.update({word:negCommon.get(word)+weight})\n",
    "            except:\n",
    "                negCommon.update({word:1})\n",
    "\n",
    "    posNum = sum(trainData['Label'] == 'POS')\n",
    "    negNum = sum(trainData['Label'] == 'NEG')\n",
    "    posPriorProb = posNum/(posNum+negNum)\n",
    "    negPriorProb = negNum/(posNum+negNum)\n",
    "    \n",
    "    testData = pd.read_csv(file,sep='\\t')\n",
    "    testData = testData[testData['Label'] != 'NEU']\n",
    "    print('******************************************************************')\n",
    "    print('Processing ',file,'with ',dataRow,' rows.')\n",
    "    predict = [calculateProba(review) for review in testData['Review']]\n",
    "    result = np.zeros([len(predict),2])\n",
    "    result = pd.DataFrame(result)\n",
    "    result[0]=[1 if tem>0 else 0 for tem in predict]\n",
    "    result[1]=list(testData['Label'])\n",
    "    result[1] = [ 1 if (tem == 'POS') else 0 for tem in list(result[1])]\n",
    "    target_names = ['class 0', 'class 1']\n",
    "    print(classification_report(result[1], result[0], target_names=target_names))\n",
    "    print(sklearn.metrics.f1_score(result[1],result[0], average='macro'))\n",
    "    scores[k] = round(sklearn.metrics.f1_score(result[1],result[0], average='macro'),4)\n",
    "    \n",
    "    f1Score += sklearn.metrics.f1_score(result[1],result[0], average='macro')\n",
    "    k += 1\n",
    "scores[-1]=f1Score/k\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8694    , 0.8748    , 0.8259    , 0.785     , 0.9121    ,\n",
       "       0.8768    , 0.8858    , 0.9236    , 0.7599    , 0.8707    ,\n",
       "       0.8932    , 0.8381    , 0.8575    , 0.8475    , 0.8059    ,\n",
       "       0.8992    , 0.9068    , 0.86071391])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=sorted(negCommon.items(), key = lambda x: x[1], reverse = True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refund  &  32.99921813917123 \\\\ \n",
      "garbage  &  32.994266353922335 \\\\ \n",
      "junk  &  32.985405264529575 \\\\ \n",
      "waste  &  32.984102163148286 \\\\ \n",
      "worst  &  32.97185301016418 \\\\ \n",
      "rma  &  32.96846494657285 \\\\ \n",
      "poorly  &  32.96194943966641 \\\\ \n",
      "terrible  &  32.95569455303623 \\\\ \n",
      "disappointed  &  32.949960906958566 \\\\ \n",
      "trash  &  32.948918425853535 \\\\ \n",
      "useless  &  32.94683346364347 \\\\ \n",
      "worthless  &  32.94057857701329 \\\\ \n",
      "awful  &  32.92520198071411 \\\\ \n",
      "defective  &  32.917904612978894 \\\\ \n",
      "return  &  32.913734688558776 \\\\ \n",
      "exchange  &  32.908001042481104 \\\\ \n",
      "respond  &  32.90487359916601 \\\\ \n",
      "poor  &  32.90409173833724 \\\\ \n",
      "disappointment  &  32.90278863695596 \\\\ \n",
      "crap  &  32.89653375032577 \\\\ \n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    print(i[0],' & ',i[1],'\\\\\\ ')\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
